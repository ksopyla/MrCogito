---
description: High-level overview of the Concept Encoder project, core architecture, short-term goals, and AI agent rules. Use this for general context.
alwaysApply: true
---

## ðŸŽ¯ Project Overview  
A research project developing "Concept Encoder and Decoder" - a transformer architecture using concept embeddings that encode information gathered from many text or audio tokens. 

The ultimate goal is to develop an efficient model that compresses long sequences (text/audio) into a small number of semantic "concept tokens" (e.g. 128 concepts from 512 tokens) and can reason over them. This approach takes inspiration from Latent Reasoning models and TRM (recursive refinement).

**Author**: Krzysztof Sopyla (krzysztof.sopyla@gmail.com, https://github.com/ksopyla)
**Project page**: https://ai.ksopyla.com/projects/concept-encoder/

## Core Architecture Idea
The Concept Encoder uses **cross-attention between a small set of "concept tokens" and the full input sequence**, separating encoding (token â†’ concept space) from decoding (concept space â†’ token predictions). 

**Key Implementations:**
- `weighted_mlm` - Weighted decoder combining concepts using position-specific weights.
- `perceiver_mlm` - Perceiver IO decoder (Input+Position queries).
- `recursive_mlm` - TRM-inspired recursive encoder: applies the same concept cross-attention layer $K$ times, allowing for test-time compute scaling and iterative concept refinement with a much smaller parameter footprint.

## Project Short-term Focus
1. **Fixing Concept Collapse:** Current pure-MLM objectives lead to rank-collapsed concepts. We are actively investigating regularized losses (VICReg, Orthogonality, T-REGS) and alternative objectives (Masked Diffusion).
2. **Concept Quality Evaluation:** We prioritize semantic benchmarks (STS-B, MRPC, QQP, MNLI, SICK, PAWS) and probing tasks over the full GLUE average.
3. **Data Scaling & Modality Transfer:** Once text concept quality is proven, we will scale the dataset and introduce an audio-to-concept adapter for speech-to-speech tasks.

## `agent_memory/` vs `docs/` Folder
- **`agent_memory/`**: Use this for all *temporary* and *intermediate* files generated during agent sessions (e.g., scratchpads, raw json dumps of concept analysis, WIP code snippets).
- **`docs/`**: Use the numbered hierarchical structure here for *permanent* research knowledge:
  - Update `docs/2_Experiments_Registry/master_experiment_log.md` immediately after any training or evaluation run finishes.
  - Put new actionable tasks in `docs/1_Strategy_and_Plans/active_todos.md`.

## Engineering Traceability (MANDATORY)
After every architectural change or completed implementation:
1. Update `CHANGELOG.md` (repo root) â€” dated entry, what changed, why.
2. Add `git_tag` to any new row in `master_experiment_log.md`.
3. All training scripts must call `get_git_info()` from `training/utils_training.py`
   and include the result in `wandb.init()` config.
4. Use the **engineer-scribe** skill (`~/.cursor/skills-cursor/engineer-scribe/SKILL.md`)
   for full checklist and conventions.
