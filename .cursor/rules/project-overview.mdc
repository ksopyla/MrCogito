---
description: High-level overview of the Concept Encoder project, core architecture, and AI agent rules.
alwaysApply: true
---

## Project Overview
"Concept Encoder and Decoder" — a transformer architecture that compresses long sequences (text/audio) into a small set of semantic "concept tokens" (e.g. 128,256, 512 concepts from 4-64 times longer tokens) via cross-attention, then decodes from concept space to token predictions.

**Author**: Krzysztof Sopyla (krzysztof.sopyla@gmail.com, https://github.com/ksopyla)
**Project page**: https://ai.ksopyla.com/projects/concept-encoder/

## Key Implementations
- `weighted_mlm` — Weighted decoder combining concepts using position-specific weights.
- `perceiver_mlm` — Perceiver IO decoder (Input+Position queries).
- `recursive_mlm` — TRM-inspired recursive encoder: same cross-attention layer applied K times.
- `diffusion_mlm` — Masked diffusion decoder (AdaLN-Zero).

## Key Directories
- `nn/` — Model implementations, `loss_manager.py`
- `training/` — Training scripts, data collators
- `evaluation/` — benchmark eval scripts
- `analysis/` — `run_concept_analysis.py` (effective rank, cosine similarities)

## `agent_memory/` vs `docs/`
- **`agent_memory/`**: Temporary/intermediate files (scratchpads, raw JSON, WIP).
- **`docs/`**: Permanent research knowledge (numbered hierarchy). Update `master_experiment_log.md` after every run. Actionable tasks go in `active_todos.md`.
