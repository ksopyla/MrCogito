---
alwaysApply: true
---
## ðŸŽ¯ Project Overview  
A research project developing "Concept Encoder and Decoder" - a transformer architecture using concept embeddings that encode information gathered from many text or audio tokens. The ultimate goal is to develop a model that can understand and generate text and audio in a coherent way.

**Repository**: https://github.com/ksopyla/MrCogito  
**Author**: Krzysztof Sopyla  
**Email**: krzysztof.sopyla@gmail.com  
**LinkedIn**: https://www.linkedin.com/in/krzysztof-sopyla/  
**Twitter**: https://twitter.com/ksopyla  
**GitHub**: https://github.com/ksopyla  
**Blog**: https://ai.ksopyla.com  
**Project page**: https://ai.ksopyla.com/projects/concept-encoder/

## Core Architecture Idea
The Concept Encoder uses **cross-attention between a small set of "concept tokens" and the full input sequence**, allowing the model to compress information from many tokens into fewer concept embeddings. This enables efficient long-context processing with ~1000x memory reduction vs standard self-attention. The architecture separates encoding (token â†’ concept space) from decoding (concept space â†’ token predictions).

**Model variants implemented:**
- `weighted_mlm` - Simplified weighted approach using learned position-specific weights (currently recommended for experiments)
- `perceiver_mlm` - Perceiver IO style decoding with Input+Position queries
- `perceiver_posonly_mlm` - Perceiver IO decoding with Position-only queries
- `concept_mlm` - Original attention-based decoding
- `sim_matrix_mlm` - Similarity matrix variant

## Project Short-term Goals
- Develop a transformer architecture using concept-level token masking for better text understanding and longer context processing.
- Find the best way to encode information from text or audio tokens into concept embeddings.
- Find the best tokenization strategy for text or audio tokens.
- Train the concept encoder via MLM task as a quick way to verify the concept idea is working.
- Evaluate the concept encoder on the GLUE benchmark and other benchmarks.
- Find the best way to decode concept embeddings back into text or audio tokens.
- Put the concept encoder and decoder together and train on text or audio tokens.

## Key Technical Details
- **Training task**: Masked Language Modeling (MLM) on Wikitext-103
- **Evaluation**: GLUE benchmark (MRPC, SST-2, MNLI, QQP, etc.)
- **Base model**: ModernBERT backbone (for tokenizer and pretrained weights)
- **Training framework**: HuggingFace Transformers + Accelerate
- **Experiment tracking**: Weights & Biases (project name: "MrCogito")
- **Loss functions**: MLM cross-entropy + optional auxiliary losses (orthogonality, uniformity, VICReg)

## `agent_memory/` Folder - AI Working Space
Use `agent_memory/` (project root) for all temporary and intermediate files generated during agent sessions:
- Intermediate plans and thoughts
- Summaries of research findings
- Code snippets and experiment notes
- Future reference items (links, commands, dataset info)
- Treat it as a personal knowledge base - keep clean and organized

## Top-level Project Structure
```
MrCogito/
â”œâ”€â”€ nn/           - Core model implementations (ConceptEncoder variants)
â”œâ”€â”€ training/     - MLM training, GLUE evaluation, dataset preprocessing
â”œâ”€â”€ scripts/      - PowerShell (Windows) and bash (Linux) training/eval scripts
â”œâ”€â”€ tests/        - Unit tests and verification scripts
â”œâ”€â”€ playground/   - Prototyping, Jupyter notebooks, experiments
â”œâ”€â”€ analysis/     - Model health checks, concept analysis, tokenizer evaluation
â”œâ”€â”€ docs/         - Research notes, literature reviews, experiment results
â”œâ”€â”€ agent_memory/ - AI agent working space for intermediate files
â”œâ”€â”€ Cache/        - Downloaded models, datasets, tokenizers, eval reports (gitignored)
â”œâ”€â”€ wandb/        - WandB training logs (gitignored)
â””â”€â”€ pyproject.toml - Poetry project definition (Python 3.12, CUDA 12.8)
```
