---
description: python pip poetry computing environments remote: Odra and Polonez Ubuntu. Servers setup, GPUs, memory, python version, CUDA compute capabilities
alwaysApply: false
---

## Remote Odra Server Environment

- **OS**: Ubuntu 22.04
- **GPU**: 3x RTX 3090, each with 24GB VRAM (CUDA compute capability 8.6)
- **CPU**: AMD Ryzen Threadripper 1900X 8-Core Processor (16 threads)
- **Memory**: 96GB RAM
- **Hostname**: odra
- **Project root**: `/home/ksopyla/dev/MrCogito`
- **Python**: 3.12 (via Poetry)
- **CUDA**: 12.x (RTX 3090 requires >= 11.0)
- **Multi-GPU training**: 3 GPUs with `accelerate` + NCCL backend

## Remote Polonez Server Environment

- **OS**: Ubuntu 22.04
- **GPU**: 4x RTX 3090, each with 24GB VRAM (CUDA compute capability 8.6)
- **CPU**: AMD Ryzen Threadripper 3970X 32-Core Processor (64 threads)
- **Memory**: 256GB RAM
- **Hostname**: polonez
- **Project root**: `/home/ksopyla/dev/MrCogito`
- **Python**: 3.12 (via Poetry)
- **CUDA**: 12.x
- **Multi-GPU training**: 4 GPUs with `accelerate` + NCCL backend
- **Estimated training time (L6 model, 30 epochs)**: ~36-48h on 4x RTX 3090

## RunPod Cloud Environment (on-demand)

- **GPU**: Variable (configured per pod)
- **Project root**: `/workspace/MrCogito`
- **Poetry home**: `/workspace/poetry/`
- **Poetry cache**: `/workspace/poetry/cache`
- **HuggingFace cache**: `/workspace/hf_home/`
- **HF datasets cache**: `/workspace/hf_home/datasets/`
- **Setup script**: `scripts/setup_runpod.sh` - copies SSH keys, sets env vars in `~/.bashrc`

## Key Environment Variables (all servers)

```bash
export HF_HOME="/path/to/hf_home/"            # HuggingFace model/dataset cache
export HF_DATASETS_CACHE="/path/to/datasets/" # Dataset cache override
export WANDB_API_KEY="..."                    # WandB tracking (from .env file)
export NCCL_TIMEOUT=3600                      # 1-hour NCCL timeout for multi-GPU
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export OMP_NUM_THREADS=8
export NCCL_SOCKET_IFNAME=^docker0,lo         # Exclude problematic network interfaces
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1
```

## Running Training on Remote Servers

**Single-run (one model type):**
```bash
bash scripts/train_mlm_multigpu_perceiver.sh
```

**Sequential training of all 3 model types:**
```bash
for TYPE in weighted_mlm perceiver_posonly_mlm perceiver_mlm; do
  sed -i "s/^MODEL_TYPE=.*/MODEL_TYPE=\"$TYPE\"/" scripts/train_mlm_multigpu_perceiver.sh
  bash scripts/train_mlm_multigpu_perceiver.sh
done
```

**GLUE evaluation:**
```bash
bash scripts/evaluate_concept_encoder_glue.sh
```

## Multi-GPU Key Details

- Framework: `accelerate` with DDP (DistributedDataParallel) and NCCL backend
- **Important bug**: Do NOT use `.expand()` in model forward pass for distributed training — use `.repeat()` instead to avoid NCCL gradient synchronization deadlocks (see `docs/debugging/nccl_timeout_fix_2025-11-11.md`)
- CUDA 12.8 has a known compiler bug affecting RTX 5090 (Blackwell/SM120) — fixed in CUDA 12.9.1+; RTX 3090 (Ampere) is not affected
- Effective batch size = `PER_DEVICE_BATCH_SIZE * NUM_GPUS * GRADIENT_ACCUMULATION_STEPS`

## CUDA Diagnostics

If experiencing GPU/CUDA issues on Linux servers, run:
```bash
bash scripts/diagnose_cuda.sh
```

## Poetry on Remote Servers

```bash
# Install poetry (if not installed)
curl -sSL https://install.python-poetry.org | python3 -

# Install dependencies
cd /home/ksopyla/dev/MrCogito
poetry install

# Run python in poetry env
poetry run python training/mlm_training.py [args]

# Or activate shell
poetry shell
```
