---
description: Research project deep-dive and experiemnt tracking and management - detailed folder structure, evaluation protocols, model implementations, training logs, and latest research directions.
globs: 
alwaysApply: false
---

# Concept Encoder Research Project Guide

When doing research and development, follow the guidelines in `.cursor/rules/ai-researcher.mdc` ‚Äî senior research scientist mindset, empirical validation focus, use of HuggingFace MCP tools and WebSearch, and mandatory experiment documentation.

## üß† Core Model Implementations (`nn/`)
| File | Description |
|---|---|
| `concept_encoder.py` | Base `ConceptEncoder` architecture (cross-attention) |
| `concept_encoder_weighted.py` | `ConceptEncoderForMaskedLMWeighted` (learned position-specific weights) |
| `concept_encoder_perceiver.py` | Perceiver IO decoding: `ConceptEncoderForMaskedLMPerceiver`, `ConceptEncoderForSequenceClassificationPerceiver`, `ConceptEncoderForSequenceClassificationViaDecoder` |
| `concept_encoder_recursive.py` | **New**: TRM-style recursive concept encoder (weight-tied shared layer applied K times) |
| `concept_encoder_recursive_mlm.py` | **New**: `RecursiveConceptEncoderForMaskedLM` (recursive encoder + perceiver decoder) |
| `loss_manager.py` | Extensible loss manager (`OrthogonalityLoss`, `UniformityLoss`, `VICRegLoss`, `KendallGalWeighting`) |

## üõ†Ô∏è Training & Evaluation (`training/`, `evaluation/`)
- `training/mlm_training.py` ‚Äî Main MLM training script (HuggingFace Trainer).
- `evaluation/evaluate_model_on_glue.py` ‚Äî Fine-tunes and evaluates on GLUE tasks (MRPC, QQP, STS-B, MNLI prioritized).
- `evaluation/evaluate_on_benchmark.py` ‚Äî Beyond-GLUE eval (SICK, PAWS).
- `analysis/run_concept_analysis.py` ‚Äî Inspects concept collapse, effective rank, and cosine similarities.

**MLM Training (Linux servers):**
```bash
bash scripts/train_mlm_multigpu_perceiver.sh
bash scripts/train_recursive_mlm.sh
```

**Evaluation Usage (Linux):**
```bash
bash scripts/evaluate_concept_encoder_glue.sh # runs concept-relevant tasks (MRPC, QQP, STS-B, MNLI)
bash scripts/evaluate_concept_encoder_sick.sh
bash scripts/evaluate_concept_encoder_paws.sh
```

## üìú Key Scripts (`scripts/`)
- **Bash (Linux)**: `train_mlm_multigpu_perceiver.sh`, `train_recursive_mlm.sh`, `train_diffusion_multigpu.sh`, `evaluate_concept_encoder_glue.sh`
- **PowerShell (Windows)**: `train_weighted_mlm.ps1`, `evaluate_concept_encoder_glue.ps1`, `sync_evaluation_reports.ps1` (for synchronizing reports from remote servers)

## üóÇÔ∏è Project Documentation Structure (`docs/`)
- `docs/1_Strategy_and_Plans/roadmap.md` ‚Äî Long-term vision, theories, and hypotheses.
- `docs/1_Strategy_and_Plans/active_todos.md` ‚Äî Short-term, actionable tasks (include completion dates).
- `docs/2_Experiments_Registry/master_experiment_log.md` ‚Äî **CRITICAL**: The central table tracking all training runs. **Always update this file when a new training run completes.** Include hardware, hyperparams, GLUE scores, WandB links, and **git_tag**.
- `docs/2_Experiments_Registry/run_reports/` ‚Äî Detailed analyses of individual runs (e.g., loss curve anomalies).
- `docs/3_Evaluations_and_Baselines/` ‚Äî Cross-model comparisons, scaling laws, and BERT baselines.
- `docs/4_Research_Notes/` ‚Äî Theoretical explorations and literature reviews.
- `docs/5_Archive/` ‚Äî Old plans and deprecated notes.
- `CHANGELOG.md` (repo root) ‚Äî **Engineering log**: what code changed, when, why. Update after every architectural change.

## üîó Traceability Protocol
Three documents must stay linked:
1. `CHANGELOG.md` ‚Üí git tag ‚Üí `master_experiment_log.md` (code ‚Üî run)
2. `active_todos.md` ‚Üí completion date ‚Üí `CHANGELOG.md` (task ‚Üî implementation)
3. WandB run config includes `git_commit` + `git_tag` (run ‚Üî code)

See `.cursor/rules/engineer-scribe.mdc` for the full traceability checklist.
Git tagging convention: `arch/{feature}` for code changes, `train/{run_id}` before training runs.

## üóÇÔ∏è Cache & Logs Structure (Local & Remote)
- `Cache/` ‚Äî This crucial folder stores synchronized files from remote servers (Polonez/Odra). Always check here for the latest results after running `scripts/sync_evaluation_reports.ps1`.
- `Cache/Models/` ‚Äî Downloaded HuggingFace models
- `Cache/Evaluation_reports/` ‚Äî CSV eval results synchronized from remote servers
- `Cache/Training/` ‚Äî Model checkpoints and optimizer states
- `wandb/` ‚Äî Training logs (gitignored)

## üî≠ Current Research Direction
1. **Concept Collapse Diagnosis:** The original MLM objective causes concept embeddings to collapse into a highly correlated space (effective rank ~5/128). Current priority is using regularization (`combined` losses + `fixed` weights) or alternate generative objectives (Masked Diffusion) to restore concept diversity without destroying semantics.
2. **Recursive Concept Encoder:** Inspired by the TRM paper, the `recursive_mlm` architecture ties weights across iteration steps ($K$). This naturally supports latent reasoning and test-time compute scaling with far fewer parameters (e.g. 42M vs 61M).
3. **Probing and Semantic Similarity:** We are dropping syntactic GLUE tasks (CoLA) and prioritizing STS-B (Pearson/Spearman), SICK (Relatedness), PAWS (Paraphrase), and SentEval probing tasks to explicitly measure what semantic properties survive the concept bottleneck.
4. **Audio Roadmap:** We validate architecture on text first. Once proven, we'll introduce an audio-to-concept adapter to map mel-spectrograms into the frozen text concept space, leading to a "Concept-Talker" speech-to-speech model.
