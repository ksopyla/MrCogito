---
description: Research project structure, experiment protocols, and tracking rules for the Concept Encoder project.
globs: 
alwaysApply: false
---

# Concept Encoder Research Project Guide

Follow `.cursor/rules/ai-researcher.mdc` for research mindset. Follow `.cursor/rules/engineer-scribe.mdc` for traceability.

## Project Structure

| Path | Purpose |
|---|---|
| `nn/` | Model implementations (`concept_encoder.py`, `*_perceiver.py`, `*_diffusion.py`, `*_recursive.py`, `loss_manager.py`) |
| `training/` | Training scripts (`mlm_training.py`, `train_diffusion.py`, `train_tsdae.py`, `data_collators.py`) |
| `evaluation/` | Eval scripts (`evaluate_model_on_glue.py`, `evaluate_on_benchmark.py`) |
| `analysis/` | `run_concept_analysis.py` — concept collapse, effective rank, cosine similarities |
| `scripts/` | Shell wrappers for Linux (`.sh`) and Windows (`.ps1`) |
| `docs/1_Strategy_and_Plans/` | `roadmap.md` (vision + tracks), `active_todos.md` (actionable tasks) |
| `docs/2_Experiments_Registry/` | `master_experiment_log.md` (central table), `run_reports/` (per-run analyses) |
| `docs/4_Research_Notes/` | Deep analyses: `mlm_perceiver_diagnosis_20260221.md`, `diffusion_diagnosis_20260226.md` |
| `Cache/` | Synced from remote servers: models, checkpoints, eval reports |

## Current Focus (Feb 2026)

**Goal:** Build audio conversational + reasoning model via concept bottleneck. Current milestone: prove concept quality on text (SG1).

**Training pipeline phases:** Phase 0 (self-reconstruction) → Phase 1 (prefix generation) → Phase 2 (recursive reasoning) → Phase 3 (instruction tuning).

**Concept count C scales with N:** C=128 at 512 tokens, C=8K-16K at 1M tokens. The invariant is O(C*N) << O(N^2).

**Active Track A priorities:** L6 diffusion ablation (TODO 11), ELBO fix (TODO 12), TSDAE PosOnly (TODO 10), prefix generation (TODO 13).

## Experiment Protocol (MANDATORY)

### Before Training
1. Create git tag: `arch/{feature}` for code, `train/{run_id}` before training.
2. Ensure `get_git_info()` is called in training script and passed to `wandb.init()` config.
3. Verify training config matches the TODO description.

### After Training
1. **Immediately** update `docs/2_Experiments_Registry/master_experiment_log.md` with a new row.
2. Run `analysis/run_concept_analysis.py` — check effective rank, pairwise similarity.
3. Run GLUE eval with ViaDecoder (default mode): MRPC, STS-B, QQP, MNLI.
4. Write run report in `docs/2_Experiments_Registry/run_reports/` if results are non-trivial.
5. Update `CHANGELOG.md` if any code changed.

### Evaluation Rules
- **Default classification head:** ViaDecoder (not CLS-query).
- **Priority benchmarks:** STS-B Pearson > concept rank > MRPC/QQP > PAWS/SICK > MNLI.
- **Skip:** CoLA (architectural ceiling), RTE (too noisy), SST-2 (saturated).
- **New metric:** Prefix generation loss (measures generation capability, not just similarity).

### Concept Quality Targets
- Effective rank > 50% of C (e.g., > 64/128)
- Mean pairwise similarity < 0.20
- Max pairwise similarity < 0.60
- STS-B Pearson > 0.75
- Zero-shot STS-B cosine > 0.60

## Traceability (3 linked documents)

1. `CHANGELOG.md` → git tag → `master_experiment_log.md`
2. `active_todos.md` → completion date → `CHANGELOG.md`
3. WandB run config includes `git_commit` + `git_tag`
