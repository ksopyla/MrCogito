---
description: Rules for training and evaluating on remote Polonez and Odra Ubuntu servers (SSH, multi-GPU, environment). Use when discussing remote servers, SSH, multi-GPU training, or distributed training.
alwaysApply: false
---

## Remote Server Rules
- **NEVER** copy source code via `scp`/`rsync`. ALWAYS use Git to sync code.
- **Do NOT** start training on remote servers without user permission.
- **DO** run `analysis/run_concept_analysis.py` automatically — it's fast.
- Sync eval reports locally: `.\scripts\sync_evaluation_reports.ps1`
- do not run many experiments (traning and evaluation) at once on the same server, it will cause resource contention and slow down the training, just run one experiment at a time.

## SSH Access (aliases in `~/.ssh/config`)
- `ssh polonez` — 4x RTX 3090 (24GB each), Threadripper 3970X 32-Core, 256GB RAM, port 2205
- `ssh odra` — 3x RTX 3090 (24GB each), Threadripper 1900X 8-Core, 96GB RAM, port 2203
- Project root: `/home/ksopyla/dev/MrCogito`

## Remote Paths
- HF cache: `/home/ksopyla/hf_home/`
- Checkpoints: `Cache/Training`, 
- Eval reports: `Cache/Evaluation_reports`, 
- Traning Logs: `Cache/logs/[model_type]_[configuration]_[date]_[time]` where configuration is a string of hyperparameters H512 hidden size, L6 6 layers, C128 concepts, examples: `/perceiver_mlm_H512L6C128_20260220_184029/`, `perceiver_posonly_mlm_H512L6C128_20260208_102656/`
- Training Shell logs: `Cache/logs/shell_[model_type]_[date]_[time].log`
- Evaluation Shell logs: `Cache/logs/shell_[benchmark]_eval_[date]_[time].log` eg:  `shell_glue_eval_diffusion_L2_20260225.log`
- wandb logs: `wandb/run-[date]_[time]-[run_id or name]`

## Multi-GPU (CRITICAL)
- Framework: `accelerate` with DDP + NCCL backend
- **NEVER use `.expand()` in forward pass** — use `.repeat()` to avoid NCCL deadlocks (see `docs/debugging/nccl_timeout_fix_2025-11-11.md`)
- Effective batch = `PER_DEVICE_BATCH * NUM_GPUS * GRAD_ACCUM_STEPS`

## Key Env Vars
```bash
export HF_HOME="/home/ksopyla/hf_home/"
export NCCL_TIMEOUT=3600
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export OMP_NUM_THREADS=8
```
